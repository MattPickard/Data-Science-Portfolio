{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A RAG Ensemble Pipeline Implementation**  \n",
    "\n",
    "**Overview**  \n",
    "This is an ensemble of the following RAG (Retrieval-Augmented Generation) techniques:\n",
    "- Query Rewriting\n",
    "- LLM-based Reranker (previously used cross-encoder reranker in comments)\n",
    "- Context Retrieval\n",
    "\n",
    "This pipeline uses:\n",
    "- LangChain\n",
    "- FAISS (Facebook AI Similarity Search)\n",
    "- OpenAI embeddings\n",
    "- GPT-4o-mini API\n",
    "\n",
    "**Implementation Reference**  \n",
    "[https://github.com/NirDiamant/RAG_Techniques](https://github.com/NirDiamant/RAG_Techniques)\n",
    "\n",
    "**Preprocessing**  \n",
    "I preprocessed my grandfather's memoir titled \"My Life Story\" into 10 PDFs (chapters). Each PDF was processed using Fitz into continuous strings and chunked into langchain Document objects. Metadata was added to each chunk to aid in retrieval of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from .env file that contains the OpenAI API key\n",
    "load_dotenv() \n",
    "\n",
    "# Get OpenAI API key from .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the PDF paths\n",
    "paths = [os.path.join(os.getcwd(), \"RAG Eval\", \"pdfs\", file) for file in os.listdir(os.path.join(os.getcwd(), \"RAG Eval\", \"pdfs\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdfs(paths, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Preprocesses PDFs using Fitz then encodes chunks into a vector store using OpenAI \n",
    "    embeddings while saving source and index as metadata. \n",
    "        paths: A list of paths to the PDF files.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded content of the PDFs with citations.\n",
    "    \"\"\"\n",
    "\n",
    "    all_texts = []\n",
    "\n",
    "    index = 0  # Initialize a global index to keep track of the index across all chunks\n",
    "\n",
    "    for path in paths:\n",
    "        # Open the PDF document located at the specified path\n",
    "        doc = fitz.open(path)\n",
    "        content = \"\"\n",
    "        # Iterate over each page in the document\n",
    "        for page_num in range(len(doc)):\n",
    "            # Get the current page\n",
    "            page = doc[page_num]\n",
    "            # Extract the text content from the current page and append it to the content string\n",
    "            content += page.get_text()\n",
    "        # Divide the content into chunks of specified size with overlap.\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(content):\n",
    "            end = start + chunk_size\n",
    "            chunk = content[start:end]\n",
    "            # Chunk is concatinated \n",
    "            chunks.append(Document(page_content=chunk))\n",
    "            # The start position is incremented by the chunk size minus the overlap to ensure consecutive chunks overlap.\n",
    "            start += chunk_size - chunk_overlap\n",
    "        # Extract file name from path\n",
    "        file_name = os.path.basename(path)\n",
    "        \n",
    "        # Update metadata instead of appending to page_content\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata.update({\n",
    "                \"index\": index,\n",
    "                \"source\": file_name\n",
    "            })\n",
    "            index += 1  # Increment the global index for each chunk\n",
    "\n",
    "        all_texts.extend(chunks)\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(all_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300\n",
    "chunk_overlap= 200\n",
    "# Encode the PDFs\n",
    "chunks_vector_store = encode_pdfs(paths, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vector store\n",
    "#chunks_vector_store.save_local(\"my_life_story_ensemble.json\")\n",
    "\n",
    "#load the vector store\n",
    "chunks_vector_store = FAISS.load_local(\"my_life_story_ensemble.json\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_by_index(vectorstore, target_index):\n",
    "    \"\"\"\n",
    "    Retrieve a chunk from the vectorstore based on its index in the metadata. Will be called in\n",
    "    get_contex().\n",
    "    \n",
    "    Args:\n",
    "    vectorstore (VectorStore): The vectorstore containing the chunks.\n",
    "    target_index (int): The index of the chunk to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    Optional[Document]: The retrieved chunk as a Document object, or None if not found.\n",
    "    \"\"\"\n",
    "    # Retrieve all documents from the vectorstore\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "    \n",
    "    # Search for the document with the specified index\n",
    "    for doc in all_docs:\n",
    "        if doc.metadata.get('index') == target_index:\n",
    "            return doc\n",
    "            \n",
    "    # If not found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query):\n",
    "    \"\"\"\n",
    "    Rewrites the original query to improve retrieval.\n",
    "    \n",
    "    Args:\n",
    "    original_query (str): The original user query\n",
    "    \n",
    "    Returns:\n",
    "    str: The rewritten query\n",
    "    \"\"\"\n",
    "    re_write_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "\n",
    "    # Create a prompt template for query rewriting\n",
    "    query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "    The following query is a question pertaining to George Shambaugh's life. Reword the same question in 3 very concise ways.\n",
    "\n",
    "    Original query: {original_query}\n",
    "\n",
    "    Rewritten query:\"\"\"\n",
    "\n",
    "    query_rewrite_prompt = PromptTemplate(\n",
    "        input_variables=[\"original_query\"],\n",
    "        template=query_rewrite_template\n",
    "    )\n",
    "\n",
    "    # Create an LLMChain for query rewriting\n",
    "    query_rewriter = query_rewrite_prompt | re_write_llm\n",
    "    \n",
    "    response = query_rewriter.invoke(original_query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def reranker(new_query, chunks_vector_store, rerank_top_k = 3):\\n    Retrieve and rerank documents based on the query using a cross-encoder model.\\n\\n    #Args:\\n    #query (str): The query to search for relevant documents.\\n    #chunks_vector_store: The vector store to query.\\n\\n    #Returns:\\n    #List[str]: A list of documents reranked by their relevance to the query\\n\\n    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\\n\\n    # Initial retrieval\\n    initial_docs = chunks_vector_store.similarity_search(new_query, k=10)\\n\\n    # Prepare pairs for cross-encoder\\n    pairs = [[new_query, doc.page_content] for doc in initial_docs]\\n\\n    # Get cross-encoder scores\\n    scores = cross_encoder.predict(pairs)\\n\\n    # Sort documents by score and include index metadata\\n    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\\n\\n    # Return top reranked documents with their index metadata\\n    return [doc for doc, _ in scored_docs[:rerank_top_k]]\""
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LLM-based Reranker\n",
    "class RatingScore(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a rating score for a document's relevance to a query.\n",
    "    \n",
    "    Attributes:\n",
    "    relevance_score (float): The relevance score of a document to a query.\n",
    "    \"\"\"\n",
    "    relevance_score: float = Field(..., description=\"The relevance score of a document to a query.\")\n",
    "\n",
    "def reranker(new_query, chunks_vector_store, top_n: int = 3):\n",
    "    \"\"\"\n",
    "    Reranks documents based on their relevance to a new query using an LLM model.\n",
    "    \n",
    "    Args:\n",
    "    new_query (str): The new query to search for relevant documents.\n",
    "    chunks_vector_store: The vector store to query.\n",
    "    top_n (int, optional): The number of top-ranked documents to return. Defaults to 3.\n",
    "    \n",
    "    Returns:\n",
    "    List[Document]: A list of documents reranked by their relevance to the new query.\n",
    "    \"\"\"\n",
    "    # Retrieve initial documents based on the new query\n",
    "    docs = chunks_vector_store.similarity_search(new_query, k=10)\n",
    "\n",
    "    # Define a prompt template for the LLM to rate document relevance\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"doc\"],\n",
    "        template= \"\"\"On a scale of 1-10, rate the relevance of the following chunk from \n",
    "        George Shambaugh's memoir to the query. Consider the specific context and intent \n",
    "        of the query, not just keyword matches.\n",
    "        Query: {query}\n",
    "        Document: {doc}\n",
    "        Relevance Score:\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the LLM model for rating document relevance\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
    "    \n",
    "    # Score each document based on its relevance to the new query\n",
    "    scored_docs = []\n",
    "    for doc in docs:\n",
    "        input_data = {\"query\": new_query, \"doc\": doc.page_content}\n",
    "        score = llm_chain.invoke(input_data).relevance_score\n",
    "        try:\n",
    "            score = float(re.search(r'\\b\\d+(\\.\\d+)?\\b', score))\n",
    "        except ValueError:\n",
    "            score = 0  # Default score if parsing fails\n",
    "        scored_docs.append((doc, score))\n",
    "    \n",
    "    # Sort documents by their relevance scores in descending order\n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    # Return the top N reranked documents\n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-encoder Reranker (not used in lastest enseble)\n",
    "\"\"\"def reranker(new_query, chunks_vector_store, rerank_top_k = 3):\n",
    "    Retrieve and rerank documents based on the query using a cross-encoder model.\n",
    "\n",
    "    #Args:\n",
    "    #query (str): The query to search for relevant documents.\n",
    "    #chunks_vector_store: The vector store to query.\n",
    "\n",
    "    #Returns:\n",
    "    #List[str]: A list of documents reranked by their relevance to the query\n",
    "\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # Initial retrieval\n",
    "    initial_docs = chunks_vector_store.similarity_search(new_query, k=10)\n",
    "\n",
    "    # Prepare pairs for cross-encoder\n",
    "    pairs = [[new_query, doc.page_content] for doc in initial_docs]\n",
    "\n",
    "    # Get cross-encoder scores\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    # Sort documents by score and include index metadata\n",
    "    scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return top reranked documents with their index metadata\n",
    "    return [doc for doc, _ in scored_docs[:rerank_top_k]]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(chunks_vector_store, reranked_chunks, num_neighbors: int = 4, chunk_overlap: int = 200):\n",
    "    \"\"\"\n",
    "    This function retrieves the context (surrounding chunks) of the reranked chunks \n",
    "    and concatonates them together accounting for overlap.\n",
    "\n",
    "    Args:\n",
    "        chunks_vector_store: The vector store to query.\n",
    "        reranked_chunks: The reranked chunks to retrieve the context for.\n",
    "        num_neighbors: The number of neighboring chunks to retrieve.\n",
    "        chunk_overlap: The amount of overlap between neighboring chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of context sequences for the reranked chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    result_sequences = []\n",
    "\n",
    "    for chunk in reranked_chunks:\n",
    "        current_index = chunk.metadata.get('index', None)\n",
    "        if current_index is None:\n",
    "            continue\n",
    "\n",
    "        # Determine the range of chunks to retrieve\n",
    "        start_index = max(0, current_index - num_neighbors)\n",
    "        end_index = current_index + num_neighbors + 1  # +1 because range is exclusive at the end\n",
    "\n",
    "        # Retrieve all chunks in the range\n",
    "        neighbor_chunks = []\n",
    "        for i in range(start_index, end_index):\n",
    "            neighbor_chunk = get_chunk_by_index(chunks_vector_store, i)\n",
    "            if neighbor_chunk:\n",
    "                neighbor_chunks.append(neighbor_chunk)\n",
    "\n",
    "        # Check if neighbor_chunks is empty\n",
    "        if not neighbor_chunks:\n",
    "            continue  # Skip to the next chunk if no neighbors found\n",
    "\n",
    "        # Concatenate chunks, accounting for overlap\n",
    "        concatenated_text = neighbor_chunks[0].page_content\n",
    "        for i in range(1, len(neighbor_chunks)):\n",
    "            current_chunk = neighbor_chunks[i].page_content\n",
    "            overlap_start = max(0, len(concatenated_text) - chunk_overlap)\n",
    "            concatenated_text = concatenated_text[:overlap_start] + current_chunk\n",
    "\n",
    "        result_sequences.append(concatenated_text)\n",
    "\n",
    "    return result_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    \"\"\"\n",
    "    Model to generate an answer to a query based on a given context.\n",
    "    \n",
    "    Attributes:\n",
    "        answer_based_on_content (str): The generated answer and citation based on the context.\n",
    "    \"\"\"\n",
    "    answer_based_on_content: str = Field(description=\"Generates an answer and [citation] to a query based on a given context.\")\n",
    "    \n",
    "def create_question_answer_from_context_chain(llm):\n",
    "    # Initialize the ChatOpenAI model with specific parameters\n",
    "    question_answer_from_context_llm = llm\n",
    "\n",
    "    # Define the prompt template for chain-of-thought reasoning\n",
    "    question_answer_prompt_template = \"\"\" \n",
    "    You are querying a memior called \"My Life Story\" written by George Shambaugh.\n",
    "    For the question below, provide a concise but suffice answer. If you don't know, only write \"The RAG retrieval was unable to provide sufficient context\":\n",
    "    {context}\n",
    "    Question\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a PromptTemplate object with the specified template and input variables\n",
    "    question_answer_from_context_prompt = PromptTemplate(\n",
    "        template=question_answer_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Create a chain by combining the prompt template and the language model\n",
    "    question_answer_from_context_cot_chain = question_answer_from_context_prompt | question_answer_from_context_llm.with_structured_output(\n",
    "        QuestionAnswerFromContext)\n",
    "    return question_answer_from_context_cot_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_context(question, context, question_answer_from_context_chain):\n",
    "    \"\"\"\n",
    "    Answer a question using the given context by invoking a chain of reasoning.\n",
    "\n",
    "    Args:\n",
    "        question: The question to be answered.\n",
    "        context: The context to be used for answering the question.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the answer, context, and question.\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "    output = question_answer_from_context_chain.invoke(input_data)\n",
    "    answer = output.answer_based_on_content\n",
    "    return {\"answer\": answer, \"context\": context, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_context(context):\n",
    "    \"\"\"\n",
    "    Display the contents of the provided context list.\n",
    "\n",
    "    Args:\n",
    "        context (list): A list of context items to be displayed.\n",
    "\n",
    "    Prints each context item in the list with a heading indicating its position.\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(context):\n",
    "        print(f\"Context {i + 1}:\")\n",
    "        print(c)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RAG(original_query):\n",
    "    \"\"\"\n",
    "    Test the Retrieval-Augmented Generation (RAG) process with a given query. It also prints the context chunks retrieved from the vector store.\n",
    "\n",
    "    Args:\n",
    "        original_query (str): The query to be tested against the vector store created from my Grandfather's memoir.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer generated by the language model based on the retrieved context.\n",
    "    \"\"\"\n",
    "    # Rewrite the original query to enhance its retrieval capabilities\n",
    "    new_query = rewrite_query(original_query)\n",
    "    # Rerank chunks from the vector store based on the enhanced query\n",
    "    reranked_chunks = reranker(new_query, chunks_vector_store)\n",
    "    # Extract context from the reranked chunks\n",
    "    context = get_context(chunks_vector_store, reranked_chunks)\n",
    "    # Initialize the language model with specific parameters\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=2000)\n",
    "    # Create a chain for question answering from context\n",
    "    question_answer_from_context_chain = create_question_answer_from_context_chain(llm)\n",
    "    # Answer the question using the context and the chain\n",
    "    answer = answer_question_from_context(original_query, context, question_answer_from_context_chain)\n",
    "    # Print the original query, enhanced query, and the response\n",
    "    print(\"Original Query:\", original_query + \"\\n\" + \"Enhanced Query:\", new_query)\n",
    "    print(\"Response:\", answer[\"answer\"], \"\\n\")\n",
    "    # Display the context chunks\n",
    "    show_context(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Who is Laura?\n",
      "Enhanced Query: 1. What is Laura's identity?\n",
      "2. Can you tell me about Laura?\n",
      "3. Who is the person named Laura?\n",
      "Response: Laura is one of George Shambaugh's children. She married Todd Pickard and initially moved to Florida for Todd's graduate school. They later returned to Indiana, where Laura worked for Eli Lilly and had two sons, Brian James and Matthew George. \n",
      "\n",
      "Context 1:\n",
      "ticultural accounting \n",
      "programming. They had one daughter (Fig 68), Cara Janelle, Eventually Cara graduated from college \n",
      "with a degree in chemistry. She met and married William Robinson and had one son, Keegan David (Fig \n",
      "69). \n",
      "Tim married Barbara (Fig 70) and remained in Madison, Ohio. He works in a research laboratory and \n",
      "Barbara is a physical therapist.                                                                                                                                        \n",
      "Laura married Todd and immediately went to Florida for Todd’s graduate school. They stayed in Florida \n",
      "for a few years, having two sons, Brian James and Matthew George (Fig 71). They moved back north to \n",
      "Indiana and got jobs there. Todd works in a computer programming company and Laura works for Eli \n",
      "Lilly. Later Brian married Juyeon Han, but that didn’t work out. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "OVERSEAS TRIPS \n",
      "Rudy and I took a few trips in the United States, but I will give more particulars on the ones we took \n",
      "over seas. I have placed them as addenda to save the eyesight of those who are not particularly \n",
      "interest\n",
      "\n",
      "\n",
      "Context 2:\n",
      "974 I served eight years on the Board of Directors of the Wayne-Holmes Counties Mental \n",
      "Health Center. I was then asked to serve on the Wayne County Mental Health & Recovery Board for six \n",
      "years. I chaired both Boards for two terms. This is the time when our three kids were graduating from \n",
      "high school and going to college. \n",
      " \n",
      "Amy went to Miami University in Ohio, majoring in psychology. Tim went to Ohio State University \n",
      "majoring in Entomology. Laura went to Bowling Green University majoring in marine biology. Laura \n",
      "found a soul-mate there and married Todd Pickard before moving to Florida to graduate school. Amy \n",
      "decided to get a Masters in Social Work at Ohio State University and found husband Kirk Markovitz \n",
      "amongst a few thousand students, When Tim graduated with his bachelor’s degree, Amy got her MSW \n",
      "and Kirk got his second bachelor’s degree (horticulture and accounting). I represented our Department \n",
      "in the academic procession resulting in a lot of robes for our family that day (Fig 67). \n",
      " \n",
      "I was a member of the Subcommittee on Classical Music In Wooster, Ohio. We were able t\n",
      "\n",
      "\n",
      "Context 3:\n",
      "the fall, Amy started first grade and Tim began afternoon kindergarten. Rudy became a Room \n",
      "Mother and volunteer librarian. One morning Tim and Laura were outside playing before Tim had to \n",
      "come in the get ready for school. Rudy saw two urchins playing in the mud in the fields across the road \n",
      "and painting the Kauffman’s mailbox with mud. It was funny until she realized that the urchins were \n",
      "hers (all hers), Laura and Tim, covered with mud and very happy. Rudy took a picture of them before \n",
      "they had to wash the mud off the mailbox (Fig 57). \n",
      " \n",
      "Just about time for Laura to start to school, it was decided she needed glasses. She got a pair with \n",
      "beautiful plastic frames called “Cotton Candy”. The only problem was that Laura took them off \n",
      "frequently and forgot where she laid them. Then the hunt was on to find them. One day we could not \n",
      "find them - ever. Rudy took Laura to get another pair of glasses. Months later when I went to the \n",
      "basement to get frozen food from our deep freezer, I found Laura’s glasses in the freezer, next to an \n",
      "opened bag of chocolates. Mystery Solved! – Well…a\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_RAG(\"Who is Laura?\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
