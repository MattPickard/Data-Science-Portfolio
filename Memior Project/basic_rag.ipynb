{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Basic RAG Pipeline Implementation**  \n",
    "\n",
    "**Overview**  \n",
    "This is a basic RAG (Retrieval-Augmented Generation) pipeline implementation using:\n",
    "- PyPDFLoader and RecursiveCharacterTextSplitter\n",    
    "- LangChain\n",
    "- FAISS (Facebook AI Similarity Search)\n",
    "- OpenAI embeddings\n",
    "- GPT-4o-mini API\n",
    "\n",
    "**Implementation Reference**  \n",
    "[https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb)\n",
    "\n",
    "**Preprocessing**  \n",
    "My grandfather's memoir titled \"My Life Story\" was split into 10 PDFs (chapters). Each PDF was processed using PyPDFLoader and chunked with RecursiveCharacterTextSplitter then cleaned of tab characters. A citation to the source chapter was appended to the end of each chunk to aid in retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from .env file that contains the OpenAI API key\n",
    "load_dotenv() \n",
    "\n",
    "# Get OpenAI API key from .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of the PDF paths\n",
    "paths = [os.path.join(os.getcwd(), \"RAG Eval\", \"pdfs\", file) for file in os.listdir(os.path.join(os.getcwd(), \"RAG Eval\", \"pdfs\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
    "\n",
    "    Args:\n",
    "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
    "\n",
    "    Returns:\n",
    "        The modified list of documents with tab characters replaced by spaces.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdfs(paths, chunk_size, chunk_overlap):\n",
    "    \"\"\"\n",
    "    Encodes multiple PDFs into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        paths: A list of paths to the PDF files.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded content of the PDF chunks with appended citations.\n",
    "    \"\"\"\n",
    "\n",
    "    all_cleaned_texts = []\n",
    "\n",
    "    for path in paths:\n",
    "        # Load PDF documents\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "        )\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "        # Extract file name from path\n",
    "        file_name = os.path.basename(path)\n",
    "\n",
    "        # Append document citation to the end of each chunk\n",
    "        for text in cleaned_texts:\n",
    "            text.page_content = text.page_content + f\" [Source: {file_name}]\"\n",
    "\n",
    "        all_cleaned_texts.extend(cleaned_texts)\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = get_langchain_embedding_provider(EmbeddingProvider.OPENAI)\n",
    "\n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(all_cleaned_texts, embeddings)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the PDFs\n",
    "chunks_vector_store = encode_pdfs(paths, chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the vector store\n",
    "#chunks_vector_store.save_local(\"basic_rag_citation.json\")\n",
    "\n",
    "#load the vector store\n",
    "chunks_vector_store = FAISS.load_local(\"basic_rag_citation.json\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever\n",
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    \"\"\"\n",
    "    Model to generate an answer to a query based on a given context.\n",
    "    \n",
    "    Attributes:\n",
    "        answer_based_on_content (str): The generated answer and citation based on the context.\n",
    "    \"\"\"\n",
    "    answer_based_on_content: str = Field(description=\"Generates an answer and [citation] to a query based on a given context.\")\n",
    "    \n",
    "def create_question_answer_from_context_chain(llm):\n",
    "    # Initialize the ChatOpenAI model with specific parameters\n",
    "    question_answer_from_context_llm = llm\n",
    "\n",
    "    # Define the prompt template for chain-of-thought reasoning\n",
    "    question_answer_prompt_template = \"\"\" \n",
    "    You are querying a memior called \"My Life Story\" written by George Shambaugh.\n",
    "    For the question below, provide a concise but suffice answer. If you don't know, only write \"The RAG retrieval was unable to provide sufficient context\":\n",
    "    {context}\n",
    "    Question\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a PromptTemplate object with the specified template and input variables\n",
    "    question_answer_from_context_prompt = PromptTemplate(\n",
    "        template=question_answer_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Create a chain by combining the prompt template and the language model\n",
    "    question_answer_from_context_cot_chain = question_answer_from_context_prompt | question_answer_from_context_llm.with_structured_output(\n",
    "        QuestionAnswerFromContext)\n",
    "    return question_answer_from_context_cot_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_context(question, context, question_answer_from_context_chain):\n",
    "    \"\"\"\n",
    "    Answer a question using the given context by invoking a chain of reasoning.\n",
    "\n",
    "    Args:\n",
    "        question: The question to be answered.\n",
    "        context: The context to be used for answering the question.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the answer, context, and question.\n",
    "    \"\"\"\n",
    "    input_data = {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "    output = question_answer_from_context_chain.invoke(input_data)\n",
    "    answer = output.answer_based_on_content\n",
    "    return {\"answer\": answer, \"context\": context, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_context(context):\n",
    "    \"\"\"\n",
    "    Display the contents of the provided context list.\n",
    "\n",
    "    Args:\n",
    "        context (list): A list of context items to be displayed.\n",
    "\n",
    "    Prints each context item in the list with a heading indicating its position.\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(context):\n",
    "        print(f\"Context {i + 1}:\")\n",
    "        print(c)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RAG(test_query):\n",
    "    \"\"\"\n",
    "    Test the Retrieval-Augmented Generation (RAG) process with a given query. It also prints the context chunks retrieved from the vector store.\n",
    "\n",
    "    Args:\n",
    "        test_query (str): The query to be tested against the vector store created from my Grandfather's memoir.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer generated by the language model based on the retrieved context.\n",
    "    \"\"\"\n",
    "    # Retrieve chunks related to the test query from the vector store\n",
    "    chunks = chunks_query_retriever.invoke(test_query)\n",
    "    # Extract the content of each chunks to form the context\n",
    "    context = [chunk.page_content for chunk in chunks]\n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=2000)\n",
    "    # Create a prompt template and combine with the language model\n",
    "    question_answer_from_context_chain = create_question_answer_from_context_chain(llm)\n",
    "    # Answer the question based on the retrieved context\n",
    "    answer = answer_question_from_context(test_query, context, question_answer_from_context_chain)\n",
    "    # Print the response generated by the language model\n",
    "    print(\"Response:\", answer[\"answer\"], \"\\n\")\n",
    "    # Display the context chunks retrieved from the vector store\n",
    "    show_context(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Laura Lynn Shambaugh is the daughter of Rudy and was born on August 3, 1960. She is mentioned in the context as a young girl who needed glasses and had various adventures related to them. \n",
      "\n",
      "Context 1:\n",
      "a young girl a little younger than Amy. The Archambaults next door had children of similar ages, so \n",
      "Amy and Tim had a lot of playmates. Rudy had become pregnant again, only this time her pregnancy \n",
      "was more of a problem. She was in and out of the hospital many times with various problems. At one \n",
      "point near the end of pregnancy when Rudy was in the hospital, the tissues of her mouth and throat \n",
      "started to break down in response to one of the medications she was given. It was a difficult , life-\n",
      "threatening time for Rudy. Laura Lynn Shambaugh was born August 3, 1960 in good health. Elfleda and \n",
      "Mom Eaton both came out to help the burgeoning family. Rudy’s physician decided that it would be \n",
      "dangerous for her to have another pregnancy, so soon after Laura was born, Rudy had a complete \n",
      "hysterectomy. \n",
      " \n",
      "Therein hangs another tale. While Rudy was recovering from and caring for baby Laura, Mom, Elfleda, [Source: Chapter 5.pdf]\n",
      "\n",
      "\n",
      "Context 2:\n",
      "Just about time for Laura to start to school, it was decided she needed glasses. She got a pair with \n",
      "beautiful plastic frames called “Cotton C andy”. The only problem was that Laura took them off \n",
      "frequently and forgot where she laid them. Then the hunt was on to find them. One day we could not \n",
      "find them - ever. Rudy took Laura to get another pair of glasses. Months later when I went to the \n",
      "basement to get frozen food from our deep freezer, I found Laura’s glasses in the freezer, next to a n \n",
      "opened bag of chocolates. Mystery Solved! – Well…almost. Laura’s new glasses went missing during \n",
      "the summer. We looked all over the house, but she had been playing outdoors also. We checked \n",
      "outdoors. We found the glasses in the bird feeder, safe and sound. \n",
      " \n",
      "Our family became involved in First Presbyterian Church. The kids were in Sunday School with the girls \n",
      "in Junior Choir. Rudy and I taught children Sunday School classes. I taught adult Sunday School classes [Source: Chapter 6.pdf]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_RAG(\"Who is Laura?\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
